import streamlit as st
import requests
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
import time

# –¢–æ–ø-10 –º–æ–¥–µ–ª–µ–π –ø–æ MMLU-Pro (Reasoning & Knowledge) - –¥–µ–∫–∞–±—Ä—å 2024
TOP_MODELS = [
    {
        "id": "google/gemini-2.5-pro-preview-05-06",
        "name": "Gemini 2.5 Pro Preview",
        "provider": "Google",
        "score": "90%"
    },
    {
        "id": "anthropic/claude-opus-4",
        "name": "Claude Opus 4",
        "provider": "Anthropic",
        "score": "90%"
    },
    {
        "id": "anthropic/claude-sonnet-4",
        "name": "Claude Sonnet 4",
        "provider": "Anthropic",
        "score": "88%"
    },
    {
        "id": "openai/gpt-4.1",
        "name": "GPT-4.1",
        "provider": "OpenAI",
        "score": "87%"
    },
    {
        "id": "x-ai/grok-3",
        "name": "Grok 3",
        "provider": "xAI",
        "score": "87%"
    },
    {
        "id": "deepseek/deepseek-chat",
        "name": "DeepSeek V3",
        "provider": "DeepSeek",
        "score": "86%"
    },
    {
        "id": "openai/codex-mini",
        "name": "Codex Mini",
        "provider": "OpenAI",
        "score": "86%"
    },
    {
        "id": "deepseek/deepseek-r1",
        "name": "DeepSeek R1",
        "provider": "DeepSeek",
        "score": "85%"
    },
    {
        "id": "moonshotai/kimi-k2",
        "name": "Kimi K2",
        "provider": "Moonshot",
        "score": "85%"
    },
    {
        "id": "qwen/qwen3-235b-a22b",
        "name": "Qwen3 235B",
        "provider": "Alibaba",
        "score": "84%"
    },
]

OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"


def query_model(model_id: str, prompt: str, api_key: str) -> dict:
    """–ó–∞–ø—Ä–æ—Å –∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ OpenRouter API"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://github.com/multi-ai-checker",
        "X-Title": "Multi AI Checker"
    }

    payload = {
        "model": model_id,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 2048,
    }

    start_time = time.time()
    try:
        response = requests.post(
            OPENROUTER_API_URL,
            headers=headers,
            json=payload,
            timeout=120
        )
        elapsed = time.time() - start_time

        if response.status_code == 200:
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            return {
                "success": True,
                "content": content,
                "time": elapsed,
                "model_id": model_id
            }
        else:
            error_msg = response.json().get("error", {}).get("message", response.text)
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ {response.status_code}: {error_msg}",
                "time": elapsed,
                "model_id": model_id
            }
    except requests.exceptions.Timeout:
        return {
            "success": False,
            "error": "–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (120 —Å–µ–∫)",
            "time": 120,
            "model_id": model_id
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "time": time.time() - start_time,
            "model_id": model_id
        }


def main():
    st.set_page_config(
        page_title="Multi AI Chat",
        page_icon="ü§ñ",
        layout="wide"
    )

    st.title("ü§ñ Multi AI Chat")
    st.markdown("**–û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å ‚Äî –æ—Ç–≤–µ—Ç—ã –æ—Ç 10 –ª—É—á—à–∏—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π**")

    # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ secrets (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) –∏–ª–∏ –∏–∑ –≤–≤–æ–¥–∞
    api_key = st.secrets.get("OPENROUTER_SECRET_KEY", "")

    # Sidebar —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")

        if api_key:
            st.success("‚úÖ API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ secrets")
        else:
            api_key = st.text_input(
                "OpenRouter API Key",
                type="password",
                help="–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞ https://openrouter.ai/keys"
            )

        st.markdown("---")
        st.subheader("üìã –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π")

        selected_models = []
        for model in TOP_MODELS:
            if st.checkbox(
                f"{model['name']} ({model['provider']}) - {model['score']}",
                value=True,
                key=f"model_{model['id']}"
            ):
                selected_models.append(model)

        st.markdown("---")
        st.markdown("""
        ### üìñ –û –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
        –≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç—ã —Ä–∞–∑–Ω—ã—Ö AI-–º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –≤–æ–ø—Ä–æ—Å.

        **API:** [OpenRouter](https://openrouter.ai)
        """)

    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    prompt = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å:",
        height=100,
        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –û–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"
    )

    col1, col2 = st.columns([1, 5])
    with col1:
        send_button = st.button("üöÄ –û—Ç–ø—Ä–∞–≤–∏—Ç—å", type="primary", use_container_width=True)
    with col2:
        parallel = st.checkbox("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã", value=True, help="–û—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–ø—Ä–æ—Å—ã –∫–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ")

    if send_button:
        if not api_key:
            st.error("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ API –∫–ª—é—á OpenRouter –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏")
            return

        if not prompt.strip():
            st.error("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å")
            return

        if not selected_models:
            st.error("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
            return

        st.markdown("---")
        st.subheader("üìä –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        results = {}
        containers = {}

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        cols = st.columns(2)

        for idx, model in enumerate(selected_models):
            col = cols[idx % 2]
            with col:
                with st.container(border=True):
                    st.markdown(f"### {model['name']}")
                    st.caption(f"Provider: {model['provider']} | Model: `{model['id']}`")
                    containers[model['id']] = st.empty()
                    containers[model['id']].info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞...")

        if parallel:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
            with ThreadPoolExecutor(max_workers=10) as executor:
                futures = {
                    executor.submit(query_model, model['id'], prompt, api_key): model
                    for model in selected_models
                }

                for future in futures:
                    model = futures[future]
                    result = future.result()
                    results[model['id']] = result

                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
                    if result['success']:
                        containers[model['id']].markdown(result['content'])
                        st.toast(f"‚úÖ {model['name']} –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ {result['time']:.1f}—Å")
                    else:
                        containers[model['id']].error(f"‚ùå {result['error']}")
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            progress = st.progress(0)
            for idx, model in enumerate(selected_models):
                result = query_model(model['id'], prompt, api_key)
                results[model['id']] = result

                if result['success']:
                    containers[model['id']].markdown(result['content'])
                else:
                    containers[model['id']].error(f"‚ùå {result['error']}")

                progress.progress((idx + 1) / len(selected_models))
            progress.empty()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        st.markdown("---")
        st.subheader("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")

        successful = sum(1 for r in results.values() if r['success'])
        failed = len(results) - successful
        avg_time = sum(r['time'] for r in results.values() if r['success']) / max(successful, 1)

        stat_cols = st.columns(4)
        stat_cols[0].metric("‚úÖ –£—Å–ø–µ—à–Ω–æ", successful)
        stat_cols[1].metric("‚ùå –û—à–∏–±–∫–∏", failed)
        stat_cols[2].metric("‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è", f"{avg_time:.1f}—Å")
        stat_cols[3].metric("üìä –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π", len(results))


if __name__ == "__main__":
    main()
